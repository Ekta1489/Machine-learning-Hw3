---
title: "Machine Learning Hw3"
author: "Ekta Chaudhary"
date: "10/04/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Weekly S&P Stock Market Data

**Description**
Weekly percentage returns for the S&P 500 stock index between 1990 and 2010.

**Format**
A data frame with 1089 observations on the following 9 variables.
Year: The year that the observation was recorded
Lag1: Percentage return for previous week
Lag2: Percentage return for 2 weeks previous
Lag3: Percentage return for 3 weeks previous
Lag4: Percentage return for 4 weeks previous
Lag5: Percentage return for 5 weeks previous
Volume: Volume of shares traded (average number of daily shares traded in billions)
Today: Percentage return for this week
Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week

```{r}
library(ISLR)
library(MASS)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
```

# (a) Produce some graphical summaries of the Weekly data.

```{r}
data(Weekly)

Weekly = Weekly[,-8]

featurePlot(x = Weekly[, 1:7], 
            y = Weekly$Direction,
            scales = list(x = list(relation = "free"), 
                        y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

# (b) Use the full data set to perform a logistic regression with Direction as the response and the five Lag variables plus Volume as predictors. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
Weekly_dat = Weekly[,-1] 
```

```{r}
glm.fit <- glm(Direction~., 
               data = Weekly_dat, 
               family = binomial)

summary(glm.fit)
```
# Lookign at the p-values, we can say that at 5% level of significance, Lag2 is statistically significant. 

# (c) Compute the confusion matrix and overall fraction of correct predictions. Briefly explain what the confusion matrix is telling you.

```{r}
test.pred.prob  <- predict(glm.fit, newdata = Weekly_dat,
                           type = "response")
test.pred <- rep("Down", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] <- "Up"

confusionMatrix(data = as.factor(test.pred),
                reference = Weekly_dat$Direction,
                positive = "Up")
```

* The Sensitivity is 92.07 % i.e., 92.07 % of True positives were predicted correctly. 
* The Specificity is 11.16 % i.e., 11.16 % of True Negatives were predicted correctly.
* The PPV is 56.43 % i.e., the precision is 56.43 %.
* The Kappa value is low at 0.035.

# (d) Plot the ROC curve using the predicted probability from logistic regression and report the AUC.

```{r}
roc.glm <- roc(Weekly_dat$Direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

# The AUC is 0.554

# (e) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors. Plot the ROC curve using the held out data (that is, the data from 2009 and 2010) and report the AUC.

```{r}
train_dat = (Weekly$Year < 2009)
Weekly_dat2 = Weekly_dat[!train_dat, 1:2]
Direction_new = Weekly_dat$Direction[!train_dat]
```

```{r}
glm.fit <- glm(Direction~ Lag1+Lag2, 
               data = Weekly_dat, 
               family = binomial, 
               subset = train_dat)

glm.probs = predict(glm.fit, Weekly_dat2, type = "response")

test.pred <- rep("Down", length(glm.probs))
test.pred[glm.probs > 0.5] <- "Up"
```

```{r}
roc.glm <- roc(Direction_new, glm.probs)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```
# The AUC is 0.556

# (f) Repeat (e) using LDA and QDA

# Using LDA

```{r}
lda.fit <- lda(Direction ~ Lag1+Lag2, data = Weekly_dat,
               subset = train_dat)
plot(lda.fit)
```
# Evaluating the test set performance using ROC.

```{r}
lda.pred <- predict(lda.fit, newdata = Weekly_dat2)

roc.lda <- roc(Direction_new, lda.pred$posterior[,2], 
               levels = c("Down", "Up"))

plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
```
# The AUC for LDA is 0.557

# Using QDA

```{r}
# usING qda() in MASS
qda.fit <- qda(Direction~ Lag1 + Lag2, data = Weekly_dat,
               subset = train_dat)

qda.pred <- predict(qda.fit, newdata = Weekly_dat2)

roc.qda <- roc(Direction_new, qda.pred$posterior[,2], 
               levels = c("Down", "Up"))

plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
```

# The AUC for QDA is 0.529.

# (g) Repeat (e) using KNN. Briefly discuss your results.

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

```{r, warning=FALSE}
set.seed(1)
model.knn <- train(x = Weekly_dat[train_dat,1:2],
                   y = Weekly_dat$Direction[train_dat],
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200, by = 5)),
                   trControl = ctrl)

ggplot(model.knn)
```
```{r}
knn.pred <- predict(model.knn,newdata = Weekly_dat2, type = "prob")[,2]
roc.knn <- roc(Direction_new, knn.pred)
plot(roc.knn, legacy.axes = TRUE, print.auc = TRUE)
```
# The AUC is 0.563

# Comparing the results:

```{r}
auc <- c(roc.glm$auc[1], roc.glm$auc[1], roc.lda$auc[1], roc.knn$auc[1])

plot(roc.glm, legacy.axes = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(roc.knn, col = 6, add = TRUE)
modelNames <- c("glm","lda","qda","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
col = 1:6, lwd = 2)
```


# The KNN appears to be the best model since the AUC is highest for KNN.